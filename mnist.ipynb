{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16734994",
   "metadata": {},
   "source": [
    "# MNIST With Pytorch: First steps\n",
    "In this notebook, we aim to learn the basics of PyTorch for computer vision using the famous MNIST dataset—an image classification task involving handwritten digits (0 to 9). \n",
    "\n",
    "**We will not focus on architecture choices, but on framework implementation and concepts understanding**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d8f86",
   "metadata": {},
   "source": [
    "## The Data\n",
    "To pull the data, we need to go through the following steps:\n",
    "1. Download the dataset;\n",
    "2. Divide in training and test data(for further validation);\n",
    "3. Preprocessing;\n",
    "\n",
    "### 1. Download the dataset\n",
    "MNIST is alwready availabel with *torchvision.datasets*. It can be downloaded with the `datasets.MNIST` class. With this class, we can use a handy pipeline that downloads MNIST, saves it at a local directory, defines if it is for training or validation, and apply a preprocessing.\n",
    "\n",
    "### 2. Divide in training and test data\n",
    "It is necessary to divide the dataset into training part and test part to ensure that no test data interferes with training (causing data leakage). To do this, we can use the `DataLoader` class, passing the data, the batch size (for further training) and shuffle to training data. Shuffle is important to garantee that the model will not learn a sequential pattern of the dataset.\n",
    "\n",
    "### 3. Preprocessing\n",
    "With this dataset, we applied a pipeline with `ToTensor()` to transform image data to tensors with values between [0, 1], and `Normalize((0,), (1,))` to normalize with **mean=0** and **std=1** (a commum practice to speed up traning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eecd4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Defines transform pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0,), (1,))\n",
    "])\n",
    "\n",
    "# Download and transform\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b7663",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "We can use matplotlib to visualize some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f53edaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot images with its lables\n",
    "def show_images(images, labels):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(10, 2))\n",
    "    for img, label, ax in zip(images, labels, axes):\n",
    "        ax.imshow(img.squeeze(), cmap='gray')\n",
    "        ax.set_title(f'Label: {label}')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of images from the training data loader\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Display the first 5 images with its labels\n",
    "show_images(images[:5], labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8fe44c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700a67e",
   "metadata": {},
   "source": [
    "## Defining a Neural Network\n",
    "In Pytorch, a model (in our case a neural network) is defined by a class that inherits `nn.Module`. This class works like a blueprint for a pytorch model and requires at least `__init__()` and `foward()` method implementations.\n",
    "\n",
    "### The constructor\n",
    "In `__init__()` we can define the layers of our model. In this example, we used the following architecture:\n",
    "\n",
    "- **1º Layer:** A fully connected layer (defined by `nn.Linear` class) for feature extraction **[784 -> 128]**;<br>\n",
    "- **ReLU:** A activation function for non-linearity **[128 -> 128]**;<br>\n",
    "- **2º Layer:** A fully connected layer for final classess features **[128 -> 10]**; <br>\n",
    "- **Softmax:** A activation function to classify the image **[10 -> 10]**.\n",
    "\n",
    "### foward()\n",
    "This method defines the execution order of a inference, from raw image to softmax classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14418f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Input must be a flat tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14141296",
   "metadata": {},
   "source": [
    "## Loss Function and Optmizer\n",
    "At this section we define\n",
    "- Negative Log-Likelihood Loss as our loss function;\n",
    "- Stochastic Gradient Descent as our optmizer\n",
    "\n",
    "`optim.SGD()` receives 3 parameters:\n",
    "- `model.parameters()`, defines that every trainable parameters will be optmized;\n",
    "- `lr=0.01`, defines that the learning step size will be 0.01 (big=-convertion; small=slow);\n",
    "- `momentum=0.9`, defines how much previous gradients affects a current step (0->1 | 0% to 100%). It is important to not stop at a local minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "debcefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()  # Negative Log-Likelihood Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312719b",
   "metadata": {},
   "source": [
    "## Training \n",
    "For each epoch, a training go through foward pass making infereces, calculates the errors with the loss functions, and adjust itself through backward pass\n",
    "\n",
    "- `optmizer.zero_grad()`: resets the gradients to zero (to avoid accumulation from previous steps);\n",
    "- `model(images)`: generates predictions from the input images;\n",
    "- `criterion(outputs, labels)`: computes the loss by comparing predictions with ground truth labels;\n",
    "- `loss.backward()`: computes gradients via backpropagation;\n",
    "- `optmizer.step()`: updates model weights based on the computed gradients;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7988a5ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m      3\u001b[39m     running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PowerRanger-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PowerRanger-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PowerRanger-env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PowerRanger-env/lib/python3.12/site-packages/torchvision/datasets/mnist.py:139\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) -> Tuple[Any, Any]:\n\u001b[32m    132\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m        index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     img, target = \u001b[38;5;28mself\u001b[39m.data[index], \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[32m    143\u001b[39m     img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76151672",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34da64e",
   "metadata": {},
   "source": [
    "For each batch of test data, uses the model to predicts a class. The prediction is the highest probability at the last layer (result of softmax activation), and is compared to the annotation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c6bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 96.57%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PowerRanger-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
